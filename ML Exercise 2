#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed May  2 12:31:48 2018

@author: 
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Apr 28 14:23:02 2018

@author: 
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd 
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from matplotlib import cm
from matplotlib import rc
from matplotlib.colors import LogNorm
from scipy import optimize

data = np.loadtxt('/Users/***/Desktop/machine-learning-ex2/ex2/ex2data1.txt', delimiter = ',')
X = data[:, 0:2]
y = data[:,2]

# plot points with different markers
exg1 = X[:,0]
exg2 = X[:,1]

fig, ax = plt.subplots()
ax.scatter(exg1[y==1],exg2[y==1], marker = 'o', label = 'Admitted')
ax.scatter(exg1[y==0], exg2[y==0], marker = 'x', label = 'Not Admitted')
plt.xlabel('Exam score 1')
plt.ylabel('Exam score 2')
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)


m,n = X.shape[0], X.shape[1]
X = np.hstack((np.ones((m,1)), X))

#sigmoid function `
def sigmoid(z):
    g = 1 / (1 + np.exp(-z))
    return g
initial_theta = np.zeros((n+1,1))

def costFunction(theta, X,y, lamb):
    m,n = X.shape[0], X.shape[1]

    h = sigmoid(np.dot(X,theta))
    delta = h - y
    J_norm = - (1/m)* (np.dot(y,np.log(h)) + np.dot((1-y),np.log(1-h))) 
    J_reg = (lamb / (2*m)) *np.sum(theta**2) -(lamb / (2*m)) *np.sum(theta[0]**2)
    J = J_norm + J_reg
    
    return J
def gradient(theta, X,y, lamb):
    m,n = X.shape[0], X.shape[1]
    h = sigmoid(np.dot(X,theta))
    delta = h - y
    grad = []
    for i in range(n):
        for j in range(1,n):
            grad.append((1/m)* np.sum(X[:,i] *delta) + (lamb/m) * theta[j])

    grad = np.asarray(grad)
    return grad
test_theta = np.array((-24,0.2,0.2))

J, grad = costFunction(initial_theta,X,y, lamb = 0), gradient(initial_theta,X,y, lamb = 0)
J_test, grad_test = costFunction(test_theta,X,y, lamb = 0), gradient(test_theta,X,y, lamb = 0)
print("Cost function for initial theta: {0} \n Gradient for initial theta : {1}".format(J,grad))
print("Cost function for test theta: {0} \n Gradient for test theta : {1}".format(J_test,grad_test))

def optimizeTheta(theta,X,y,lamb):
    result = optimize.fmin(costFunction, x0=theta, args=(X, y, lamb), maxiter=400, full_output=True)
    return result[0], result[1]
theta,mincost = optimizeTheta(initial_theta, X,y, 0)
print("Cost function for optimised theta: {0} \n Theta : {1}".format(costFunction(theta,X,y,0),theta))
#finding decision boundary separating admitted/not admitted
#feature 
def mapFeature(X1,X2):
    degree = 6 
    Xlen = X1.shape[0]
    out = np.ones((Xlen,1))
    for i in range(degree):
        for j in range(i):
            out[:,i].append((X1 **(i-j)) * (X2**(j)))
    return out



def plotDecisionboundary(theta,X,y):
    Xlen = X.shape[1]    
    if Xlen <= 3:
        plot_x = np.array([np.min(X[:,1]) - 2, np.max(X[:,1] + 2)])
            #calculate decision boundary line
        plot_y =  (-1./theta[2])* (theta[1] * plot_x + theta[0])
            
        plt.plot(plot_x, plot_y,'b-',label='Decision Boundary')
        plt.legend()

    else:
        u = np.linspace(-1, 1.5, 50)
        v = np.linspace(-1, 1.5, 50) 
        ulen = u.shape[0]
        vlen = v.shape[0]
        z = np.zeros((ulen,vlen))
#evaluate z = theta* X over grid of X
        for i in range(ulen):
            for j in range(vlen):
                z[i,j] = mapFeature(u[i], v[j]) * theta
        z = np.transpose(z)
        plt.contour(u, v, z,'b-',label='Decision Boundary')
plotDecisionboundary(theta,X,y)
def predict_value(theta,X):
    #predicts values for X using threshold of 0.5 
    m = X.shape[0]
    p = sigmoid(np.dot(X,theta))

    if p >= 0.5:
        return 1
    else:
        return 0
def predict_probability(theta, X):
    m = X.shape[0]
    p = sigmoid(np.dot(X,theta))
    return p*100

X1 = np.array([1, 45, 85])
print('prediction : {}'.format(predict_value(theta,X1)))
print('prediction percentage: {}'.format(predict_probability(theta,X1)))
    
 
data1 = np.loadtxt('/Users/***/Desktop/machine-learning-ex2/ex2/ex2data2.txt', delimiter = ',')
X = data1[:, 0:2]
y = data1[:,2]

# plot points with different markers
exg1 = X[:,0]
exg2 = X[:,1]

#
fig, ax = plt.subplots()
ax.scatter(exg1[y==1],exg2[y==1], marker = 'o', label = 'y = 1')
ax.scatter(exg1[y==0], exg2[y==0], marker = 'x', label = 'y = 0')
plt.xlabel('Microchip Test 1')
plt.ylabel('Microchip Test 2')
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)


X = mapFeature(X[:,0], X[:,1])
m,n = X.shape[0], X.shape[1]

theta0 = np.zeros((n,1))
J_reg, grad_reg = costFunction(theta0,X,y,1), gradient(theta0,X,y, 1)
print(J_reg,grad_reg[0:5])
theta_reg,mincost = optimizeTheta(theta0, X,y, 1)
print(theta_reg)
theta0 = theta
plotDecisionboundary(theta0,X,y)


   



     
        
        
        
